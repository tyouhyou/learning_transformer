{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n03e3Q6ku1Hl"
      },
      "source": [
        "This is just a note for my learning of transformer, referred to articles and codes from\n",
        "\n",
        "- https://www.tensorflow.org/tutorials/text/transformer\n",
        "- https://github.com/MorvanZhou/NLP-Tutorials/blob/master/transformer.py\n",
        "- https://zhuanlan.zhihu.com/p/347904940\n",
        "- https://datawhalechina.github.io/learn-nlp-with-transformers\n",
        "- https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "All credits go to the above authors.\n",
        "\n",
        "The notes / comments are my understanding at this moment, please correct me if they are wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8r9Ua_Pb3MBk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-30 16:37:17.716110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-01-30 16:37:17.716504: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input and Embedding\n",
        "Take the vocabulary table(s) and the sequence max size information. When call on it with a sequence, return the tokenized and embedded list for next processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InputEmbedding():\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_seq_length      : int,\n",
        "        vocabularies        : list  #shape: (2, x) -> there has two vocabulary table, one for in and one for out. And the two can be the same table.\n",
        "        ):\n",
        "        self.max_s_size = max_seq_length\n",
        "        if 1 == len(vocabularies):\n",
        "            self.voc_in = vocabularies\n",
        "            self.voc_out = vocabularies\n",
        "        elif 2 == len(vocabularies):\n",
        "            self.voc_in = vocabularies[0]\n",
        "            self.voc_out = vocabularies[1]\n",
        "        else:\n",
        "            raise \"invalid vocabularies, it should have one or two 1-d list(s).\"\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \"\"\"\n",
        "        Tokenize the sequence, and embed\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZMFACy23IX"
      },
      "source": [
        "## Input embedding and Positional encoding\n",
        "\n",
        "If input is [batch_size, sequence_length], after embedding, we get a tensor of [batch_size, sequence_length, embedding_dimension].\n",
        "* The batch size is the number of batch to transformer, in human's language, the number of sentences.\n",
        "* The sequence length is the size of samples in one inputting, that is, the words number in one sentence. Since not all sentences have same length, this value should be the possible longest length.\n",
        "* The embedding dimention is decided by Word2Vec. For transformer make it 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJm_IvUN268q"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding():\n",
        "    def __init__(\n",
        "        self, \n",
        "        #vocabulary_size         : int,\n",
        "        vocabulary              : np.array,\n",
        "        max_sequence_length     : int,\n",
        "        batch_size              : int,\n",
        "        embed_dimension         : int = 512\n",
        "        ):\n",
        "        \"\"\"\n",
        "        @arg\n",
        "        embed_dimension:\n",
        "            The dimmention of embedding, or feature number.\n",
        "        batch_size:\n",
        "            If the sentences to be trained is 10, the batch_size is 10.\n",
        "        sequence_length:\n",
        "            The words size in one sentence. \n",
        "            If there has 8 words in one sentence, the value is 8. \n",
        "            However, since it's impossible for all sentences have same size, \n",
        "            this value should be set to a size of the possibe longest sentence.\n",
        "        vocabulary_size:\n",
        "            The vocabulary table size.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.e_dim  = embed_dimension\n",
        "        self.b_size = batch_size\n",
        "        self.s_size = max_sequence_length\n",
        "        self.v_size = vocabulary.size()\n",
        "        \n",
        "        # [sequence_size, embed_dimension]\n",
        "        pos = np.arange(self.s_size)[:, None]\n",
        "        positional_encoding = np.array([\n",
        "            [p / np.power(10000, (2 * i for i in np.arange(self.e_dim)[None, :] / self.e_dim))] if p !=0 \n",
        "            else np.zeros(self.e_dim)       # padding zeros for the first one\n",
        "            for p in pos\n",
        "        ])\n",
        "        \n",
        "        # The formula for calculating the positional encoding is as follows:\n",
        "        # PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        # and the formula are not applied to the padding (the first one)\n",
        "\n",
        "        positional_encoding[:, 0::2] = np.sin(positional_encoding[1:, 0::2])\n",
        "        positional_encoding[:, 1::2] = np.cos(positional_encoding[1:, 1::2])\n",
        "        \n",
        "        self.positional_encoding = tf.cast(positional_encoding, dtype=tf.float32)\n",
        "        \n",
        "        # keep the embedding layer object\n",
        "        self.embedding = keras.layers.Embedding(\n",
        "            self.v_size, # vocabulary size. Since encoder and decoder may have different v_size, left it to be decided in invoker, \n",
        "            output_dim=embed_dimension,\n",
        "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.01),\n",
        "        )\n",
        "\n",
        "    def __call__(self, vocabulary_size: int):\n",
        "        \"\"\"\n",
        "        vocabulary_size:\n",
        "            The vocabulary table dimention. \n",
        "            Say, if we have a vocabulary table which containing 1000 words, \n",
        "            this value is 1000.\n",
        "            NOTE: the vocabulary sizes of decoder input and encoder target\n",
        "                  are not necessarily to be same.\n",
        "        \"\"\"\n",
        "        return self.embedding(vocabulary_size) + self.positional_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfth32m3Aej"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN__Pa7V3IOq"
      },
      "outputs": [],
      "source": [
        "class encoder(keras.layers.Layer):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zLwJO4E2wXL"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbtQ2y3r2ze7"
      },
      "outputs": [],
      "source": [
        "class decoder():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ALe7aa_3O6p"
      },
      "source": [
        "## Transfomer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwwvHDAOfHiT"
      },
      "outputs": [],
      "source": [
        "class transformer(keras.Model):\n",
        "    def __init__(self):\n",
        "       pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "learning_transformer_tensorflow.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
