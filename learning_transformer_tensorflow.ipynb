{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n03e3Q6ku1Hl"
      },
      "source": [
        "This is just a note for my learning of transformer, referred to articles and codes from\n",
        "\n",
        "- https://www.tensorflow.org/tutorials/text/transformer\n",
        "- https://datawhalechina.github.io/learn-nlp-with-transformers\n",
        "\n",
        "All credits go to the above authors.\n",
        "\n",
        "The notes / comments are my understanding at this moment, please correct me if they are wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r9Ua_Pb3MBk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer\n",
        "Tokenize the inputting sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary          : np.array # List[str]\n",
        "        ):\n",
        "        self.voc = vocabulary\n",
        "\n",
        "    def __call__(self, sequence_batch : np.array) -> np.array:\n",
        "        \"\"\"\n",
        "        Tokenize the sequence\n",
        "        @arg\n",
        "            sequence_batch: batch of sequences inputting.\n",
        "                            starts with <BOS> and ends with <EOS>\n",
        "                            the list wrapped in np.array is List[List[str]]\n",
        "        @return\n",
        "            np.array, shape [batch, sequence_size]\n",
        "        \"\"\"\n",
        "        # TODO 1. insert <bos> and <eos> to sequence and padding ?\n",
        "        #      2. performance issue ?\n",
        "        tk = np.array([[self.voc.index(s) for s in seq] for seq in sequence_batch])\n",
        "        return tk\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZMFACy23IX"
      },
      "source": [
        "## Embedding And Positional encoding\n",
        "\n",
        "If input is [batch_size, sequence_length], after embedding, we get a tensor of [batch_size, sequence_length, embedding_dimension].\n",
        "* The batch size is the number of batch to transformer, in human's language, the number of sentences.\n",
        "* The sequence length is the size of samples in one inputting, that is, the words number in one sentence. Since not all sentences have same length, this value should be the possible longest length.\n",
        "* The embedding dimention is decided by Word2Vec. For transformer make it 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NJm_IvUN268q"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocabulary_size         : int,\n",
        "        max_sequence_length     : int,\n",
        "        embed_dimension         : int = 512,\n",
        "        padding                 : bool = False, # in case for encoder if masking padding is desired\n",
        "        initializer             = tf.initializers.RandomNormal(0., 0.01)\n",
        "        ):\n",
        "        \"\"\"\n",
        "        @arg\n",
        "        embed_dimension:\n",
        "            The dimmention of embedding, or feature number.\n",
        "        sequence_length:\n",
        "            The words size in one sentence. \n",
        "            If there has 8 words in one sentence, the value is 8. \n",
        "            However, since it's impossible for all sentences have same size, \n",
        "            this value should be set to a size of the possibe longest sentence.\n",
        "        vocabulary:\n",
        "            The vocabulary table.\n",
        "        \"\"\"\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        \n",
        "        self.s_size = max_sequence_length\n",
        "        self.v_size = vocabulary_size\n",
        "        self.e_size = embed_dimension\n",
        "        \n",
        "        self.embedding = keras.layers.Embedding(\n",
        "            input_dim=self.v_size,\n",
        "            output_dim=self.e_size,\n",
        "            input_length=self.s_size,\n",
        "            embeddings_initializer = initializer,\n",
        "            mask_zero=True\n",
        "        )\n",
        "        \n",
        "        # make positional encoding array\n",
        "        positional_encoding = np.array(\n",
        "            [[pos / np.power(10000, 2 * i / self.e_size) for i in range(self.e_size)]\n",
        "             if (not (padding and 0 != pos)) else \n",
        "             np.zeros(self.e_size) for pos in np.arange(self.s_size)\n",
        "            ])\n",
        "\n",
        "        # The formula for calculating the positional encoding is as follows:\n",
        "        # PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "\n",
        "        positional_encoding[0:, 0::2] = np.sin(positional_encoding[0:, 0::2])\n",
        "        positional_encoding[0:, 1::2] = np.cos(positional_encoding[0:, 1::2])\n",
        "        positional_encoding = positional_encoding[None, :, :]\n",
        "        self.positional_encoding = tf.cast(positional_encoding, dtype=tf.float32)\n",
        "        \n",
        "    def call(self, batch : np.array):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch: which is expected to be the output of Tokenizer.\n",
        "            The type of batch is np.array, shape -> [batch_size, sequence_szie];\n",
        "        \n",
        "        return:\n",
        "            tf.Tensor, shape -> [batch_size, sequence_szie, embed_dimension]\n",
        "        \"\"\"\n",
        "        return self.embedding(batch) + self.positional_encoding[:, :self.s_size, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample result of PositionalEmbedding\n",
        "model = keras.Sequential()\n",
        "model.add(PositionalEmbedding(5, 4, 3, padding=True))\n",
        "model.compile()\n",
        "test_rst = model.predict(np.array([[0,1,2,3],[2,3,4,1]]))\n",
        "print(test_rst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self Attention\n",
        "Scaled Dot-Product Attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention() :\n",
        "    def __init__(self, d_model):\n",
        "        \"\"\"\n",
        "        It may be better to be implemented in Multihead-Attention directly.\n",
        "        Just to make it be the \"self attention part\" as described in the Transformer diagram.\n",
        "        Args:\n",
        "            d_model: tokenized embeded_dimention / head_num\n",
        "        \"\"\"\n",
        "        self.wq = keras.layers.Dense(d_model)\n",
        "        self.wk = keras.layers.Dense(d_model)\n",
        "        self.wv = keras.layers.Dense(d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, iq, ik, iv, mask=None):\n",
        "        \"\"\"\n",
        "        dot(Q,K) => Scale => Mask => Softmax => dot(attention_weights, V)\n",
        "        Args:\n",
        "            input -> iq, ik, iv: [sequence_szie, embeded_dimension]\n",
        "            NOTICE: In real life, the shape will be [batch, head_num, seq_size, e_dim],\n",
        "                    for multi-head attention calculation.\n",
        "                    However, when we consider the calculation itself, \n",
        "                    we can assume it as if sequence_szie, embeded_dimension.\n",
        "        \"\"\"\n",
        "        assert (self.d_model == tf.shape(iq)[-1]), \"Invalid input q\"\n",
        "        assert (self.d_model == tf.shape(ik)[-1]), \"Invalid input k\"\n",
        "        assert (self.d_model == tf.shape(iv)[-1]), \"Invalid input v\"\n",
        "\n",
        "        q = self.wq(iq)\n",
        "        k = self.wk(ik)\n",
        "        v = self.wv(iv)\n",
        "\n",
        "        # FORMULAR -> Z = Softmax(QK(t)/d(k)^0.5)V\n",
        "        \n",
        "        # transpose the last 2 dimention of k. (..., seq_size, e_dim) => (..., e_dim, seq)\n",
        "        qkt = tf.matmul(q, k, transpose_b=True)     # Q dot K(transpose)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)   # K embeded dimention (tensor)\n",
        "        scale = qkt / tf.math.sqrt(dk)\n",
        "\n",
        "        # mask\n",
        "        if mask is not None:\n",
        "            scale += mask * (-1e10)\n",
        "\n",
        "        # return values\n",
        "        aw = tf.nn.softmax(scale, axis=-1)          # attention weights\n",
        "        sv = tf.matmul(aw, v)                       # scores vector     (result of attention)\n",
        "        \n",
        "        return sv, aw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MultiHead Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, head_num):\n",
        "        \"\"\"\n",
        "        Multi head attention\n",
        "        Args:\n",
        "            head_num:  number of heads\n",
        "        \"\"\"\n",
        "        super.__init__(MultiHeadAttention, self)\n",
        "        self.head_num\n",
        "        \n",
        "    def call(self, seq_batch: tf.tensor, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            seq_batch: positional-embeded sequences. shape = [batch_size, sequence_szie, embed_dimension]\n",
        "        \"\"\"\n",
        "        ishape = seq_batch.get_shape()\n",
        "        assert (3, len(ishape)), \"The intput should be a tensor of [batch_size, seq_size, embedded_dimention]\"\n",
        "        \n",
        "        batch_size = ishape[0]\n",
        "        seq_num = ishape[1]\n",
        "        e_dim = ishape[2]                                                           # embeded_dimention\n",
        "\n",
        "        d_model = e_dim / self.head_num                                             # d_model to self-attention (embedded_dimention / head_num)\n",
        "        assert (e_dim == d_model * self.head_num), \"The dimention should be divisible of head number.\"\n",
        "\n",
        "        q = k = v = tf.reshape(seq_batch, (batch_size, -1, seq_num, d_model))\n",
        "        attention = SelfAttention(d_model)\n",
        "        scores, weights = attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scores, perm=[0,2,1,3])                     #[batch_size, seq_size, head_num, d_model]\n",
        "        connect_attention = tf.reshape(scaled_attention, (batch_size, -1, e_dim))   #[batch_size, seq_size, e_dim]\n",
        "        output = self.dense(connect_attention)                                      #[batch_size, seq_size, e_dim]\n",
        "\n",
        "        return output, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfth32m3Aej"
      },
      "source": [
        "## Encoder\n",
        "After self-attention, the output is forwarded to normalization and dropout layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN__Pa7V3IOq"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zLwJO4E2wXL"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbtQ2y3r2ze7"
      },
      "outputs": [],
      "source": [
        "class Decoder():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ALe7aa_3O6p"
      },
      "source": [
        "## Transfomer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwwvHDAOfHiT"
      },
      "outputs": [],
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(self):\n",
        "       pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "learning_transformer_tensorflow.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
