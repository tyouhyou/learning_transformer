{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"learning_transformer_tensorflow.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMHksWyn6HaDyfne7VeXxHe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is just a note for my learning of transformer, referred to articles and codes from\n","\n","- https://www.tensorflow.org/tutorials/text/transformer\n","- https://github.com/MorvanZhou/NLP-Tutorials/blob/master/transformer.py\n","- https://zhuanlan.zhihu.com/p/347904940\n","- https://datawhalechina.github.io/learn-nlp-with-transformers\n","\n","All credits go to the above authors.\n","\n","The notes / comments are my understanding at this moment, please correct me if they are wrong."],"metadata":{"id":"n03e3Q6ku1Hl"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"],"metadata":{"id":"8r9Ua_Pb3MBk","executionInfo":{"status":"ok","timestamp":1642897735715,"user_tz":-540,"elapsed":2654,"user":{"displayName":"张冰","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00577311045916448743"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Transfomer model"],"metadata":{"id":"3ALe7aa_3O6p"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwwvHDAOfHiT"},"outputs":[],"source":["class transformer(keras.Model):\n","    def __init__(self):\n","       pass"]},{"cell_type":"markdown","source":["## Input embedding\n","Transfer inputs to vector with positional info.\n","\n","input layer -> hidden layer -> output layer.\n","> - Mr.Wagaru: Do you understance what you are talking about?\n","> - me: After position embedding, you can get the weight of a word since the location of each word in one-hot encoder is deifferent ...\n","> - Mr.Wagaru: Do you really understand it? really understand?? really???\n","> - me: はい、はい、引き続き頑張ります。\n","\n","If input is [batch_size, sequence_length], after embedding, we get a tensor of [batch_size, sequence_length, embedding_dimension].\n","* The batch size is the number of batch to transformer, in human's language, the number of sentences.\n","* The sequence length is the size of samples in one inputting, that is, the words number in one sentence. Since not all sentences have same length, this value should be the possible longest length.\n","* The embedding dimention is decided by Word2Vec. For transformer make it 512."],"metadata":{"id":"KLZMFACy23IX"}},{"cell_type":"code","source":["class Embedding(keras.layers.Layer):\n","    def __init__(\n","        self, \n","        embed_dimension: int, \n","        batch_size     : int,\n","        sequence_length: int\n","        ):\n","        \"\"\"\n","        @arg\n","        embed_dimension:\n","            The dimmention of embedding, or feature number.\n","        batch_size:\n","            If the sentences to be trained is 10, the batch_size is 10.\n","        sequence_length:\n","            The words size in one sentence. \n","            If there has 8 words in one sentence, the value is 8. \n","            However, since it's impossible for all sentences have same size, \n","            this value should be set to a size of the possibe longest sentence.\n","        \"\"\"\n","        super(Embedding, self).__init__()\n","        self.e_dim  = embed_dimension\n","        self.b_size = batch_size\n","        self.s_size = sequence_length\n","        self.v_size = vocabulary_size\n","\n","        \n","\n","        # [sequence_size, embed_dimension]\n","        pos = np.arange(s_size)[:, None]\n","        # positional_encoding = pos / np.power(10000, 2. * np.arange(e_dim)[None, :] / e_dim)\n","        \n","\n","        positional_encoding = np.array(\n","            [\n","             [p / np.power(10000, 2 * i for i in np.arange(e_dim)[None, :] / e_dim)] if p !=0\n","             else ne.zeros(e_dim)   # padding zeros\n","             for p in pos\n","            ]\n","        )\n","        \n","        # The formula for calculating the positional encoding is as follows:\n","        # PE(pos, 2i) = sin(pos/10000^(2i/embed_dimension))\n","        # PE(pos, 2i+1) = cos(pos/10000^(2i/embed_dimension))\n","        # and the formula are not applied to the padding (the first one)\n","\n","        positional_encoding[:, 0::2] = np.sin(positional_encoding[1:, 0::2])\n","        positional_encoding[:, 1::2] = np.cos(positional_encoding[1:, 1::2])\n","        \n","        self.positional_encoding = tf.cast(positional_encoding, dtype=tf.float32)\n","        \n","        # keep the embedding layer object\n","        self.embedding = keras.layers.Embedding(\n","            input_dim, # vocabulary size. Since encoder and decoder may have different v_size, left it to be decided in invoker, \n","            output_dim=embed_dimension,\n","            embeddings_initializer=tf.initializers.RandomNormal(0., 0.01),\n","        )\n","\n","    def __call__(self, vocabulary_size: int):\n","        \"\"\"\n","        vocabulary_size:\n","            The vocabulary table dimention. \n","            Say, if we have a vocabulary table which containing 1000 words, \n","            this value is 1000.\n","            NOTE: the vocabulary sizes of decoder input and encoder target\n","                  are not necessarily to be same.\n","        \"\"\"\n","        return self.embedding(vocabulary_size) + self.positional_encoding;"],"metadata":{"id":"NJm_IvUN268q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"scfth32m3Aej"}},{"cell_type":"code","source":["class encoder(keras.layers.Layer):\n","    pass\n"],"metadata":{"id":"cN__Pa7V3IOq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"7zLwJO4E2wXL"}},{"cell_type":"code","source":["class decoder():\n","    pass"],"metadata":{"id":"LbtQ2y3r2ze7"},"execution_count":null,"outputs":[]}]}