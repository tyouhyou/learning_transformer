{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n03e3Q6ku1Hl"
      },
      "source": [
        "This is just a note for my learning of transformer, referred to articles and codes from\n",
        "\n",
        "- https://www.tensorflow.org/tutorials/text/transformer\n",
        "- https://github.com/MorvanZhou/NLP-Tutorials/blob/master/transformer.py\n",
        "- https://zhuanlan.zhihu.com/p/347904940\n",
        "- https://datawhalechina.github.io/learn-nlp-with-transformers\n",
        "- https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "All credits go to the above authors.\n",
        "\n",
        "The notes / comments are my understanding at this moment, please correct me if they are wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r9Ua_Pb3MBk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer\n",
        "Tokenize the inputting sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class Tokenizer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary          : list\n",
        "        ):\n",
        "        self.voc = vocabulary\n",
        "\n",
        "    def __call__(self, sequence_batch : List[List[str]]) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Tokenize the sequence\n",
        "        @arg\n",
        "            sequence_batch: batch of sequences inputting.\n",
        "                            starts with <BOS> and ends with <EOS>\n",
        "        \"\"\"\n",
        "        # TODO 1. insert <bos> and <eos> to sequence and padding\n",
        "        #      2. performance issue ?\n",
        "        tk = [[self.voc.index(s) for s in seq] for seq in sequence_batch]\n",
        "        return tk\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZMFACy23IX"
      },
      "source": [
        "## Embedding And Positional encoding\n",
        "\n",
        "If input is [batch_size, sequence_length], after embedding, we get a tensor of [batch_size, sequence_length, embedding_dimension].\n",
        "* The batch size is the number of batch to transformer, in human's language, the number of sentences.\n",
        "* The sequence length is the size of samples in one inputting, that is, the words number in one sentence. Since not all sentences have same length, this value should be the possible longest length.\n",
        "* The embedding dimention is decided by Word2Vec. For transformer make it 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJm_IvUN268q"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding():\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocabulary_size         : list,\n",
        "        max_sequence_length     : int,\n",
        "        embed_dimension         : int = 512\n",
        "        ):\n",
        "        \"\"\"\n",
        "        @arg\n",
        "        embed_dimension:\n",
        "            The dimmention of embedding, or feature number.\n",
        "        sequence_length:\n",
        "            The words size in one sentence. \n",
        "            If there has 8 words in one sentence, the value is 8. \n",
        "            However, since it's impossible for all sentences have same size, \n",
        "            this value should be set to a size of the possibe longest sentence.\n",
        "        vocabulary:\n",
        "            The vocabulary table.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.s_size = max_sequence_length\n",
        "        self.v_size = vocabulary_size\n",
        "        self.e_size = embed_dimension\n",
        "        \n",
        "        em = keras.layers.Embedding(\n",
        "            input_dim=self.v_size,\n",
        "            output_dim=self.e_size,\n",
        "            input_size=self.s_size,\n",
        "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.01),\n",
        "            mask_zero=True\n",
        "        )\n",
        "        \n",
        "        # make positional encoding\n",
        "        pos = np.arange(self.s_size)[:, None]\n",
        "        positional_encoding = np.array([\n",
        "            [p / np.power(10000, (2 * i for i in np.arange(self.e_dim)[None, :] / self.e_dim))] if p !=0 \n",
        "            else np.zeros(self.e_dim)       # padding zeros for the first one\n",
        "            for p in pos\n",
        "        ])\n",
        "        \n",
        "        # The formula for calculating the positional encoding is as follows:\n",
        "        # PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        # and the formula are not applied to the padding (the first one)\n",
        "\n",
        "        positional_encoding[:, 0::2] = np.sin(positional_encoding[1:, 0::2])\n",
        "        positional_encoding[:, 1::2] = np.cos(positional_encoding[1:, 1::2])\n",
        "        \n",
        "        self.positional_encoding = tf.cast(positional_encoding, dtype=tf.float32)\n",
        "        \n",
        "\n",
        "    def __call__(self):\n",
        "        # TODO: embedding compile/predict is needed?\n",
        "        return self.embedding(self.v_size) + self.positional_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfth32m3Aej"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN__Pa7V3IOq"
      },
      "outputs": [],
      "source": [
        "class encoder(keras.layers.Layer):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zLwJO4E2wXL"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbtQ2y3r2ze7"
      },
      "outputs": [],
      "source": [
        "class decoder():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ALe7aa_3O6p"
      },
      "source": [
        "## Transfomer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwwvHDAOfHiT"
      },
      "outputs": [],
      "source": [
        "class transformer(keras.Model):\n",
        "    def __init__(self):\n",
        "       pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "learning_transformer_tensorflow.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
